{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b942e0",
   "metadata": {
    "id": "e5b942e0",
    "outputId": "de69cf16-0818-4a3c-eca4-1ad2ee1dacf5"
   },
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, BartTokenizer, AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a7588-3e4f-4a38-84dd-f00978e37570",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4f933",
   "metadata": {
    "id": "ffd4f933"
   },
   "outputs": [],
   "source": [
    "directory = r\"Local Directory\\eLife\"\n",
    "train_name = os.path.join(directory, \"train.json\")\n",
    "val_name = os.path.join(directory, \"val.json\")\n",
    "test_name = os.path.join(directory, \"test.json\")\n",
    "\n",
    "# Open training data\n",
    "with open(train_name, 'r') as f:\n",
    "    data_train = json.load(f)\n",
    "\n",
    "# Open validation data\n",
    "with open(val_name, 'r') as f:\n",
    "    data_val = json.load(f)\n",
    "\n",
    "# Open test data\n",
    "with open(test_name, 'r') as f:\n",
    "    data_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edd34d-80ca-4143-879b-7a0c407d38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "\n",
    "    preprocessed_texts = []\n",
    "\n",
    "    for article in data:\n",
    "    \n",
    "        # Preprocess title\n",
    "        # Add dot at the end of the title\n",
    "        title = article[\"title\"] + \".\"\n",
    "    \n",
    "        # Preprocess sections and headings\n",
    "        article_sections = {}\n",
    "        main_body = \"\"\n",
    "        for sub_sections, heading in zip(article[\"sections\"], [heading.lower() for heading in article[\"headings\"]]):\n",
    "            sub_section_text = \" \".join(sub_sections)\n",
    "            # Add space between sections\n",
    "            main_body += sub_section_text + \" \"\n",
    "            article_sections[f\"{heading}\"] = sub_section_text\n",
    "    \n",
    "        # Preprocess abstract, summary, and keywords\n",
    "        abstract = \" \".join(article[\"abstract\"])\n",
    "        summary = \" \".join(article[\"summary\"])\n",
    "        # Add dot at the beginning and end of keywords\n",
    "        keywords = \".\" + \" \".join(article[\"keywords\"]) + \".\"\n",
    "    \n",
    "        # Combine all sections into main body\n",
    "        # Remove leading/trailing spaces\n",
    "        main_body = main_body.strip()\n",
    "    \n",
    "        # Combine all components into final data\n",
    "        preprocessed_texts.append({\n",
    "            \"abstract\": f\"\"\"{abstract}\"\"\",\n",
    "            **article_sections,\n",
    "            \"main_body\": f\"\"\"{main_body}\"\"\",\n",
    "            \"complete_text\": f\"\"\"{title} {abstract} {main_body} {keywords}\"\"\",\n",
    "            \"summary\": f\"\"\"{summary}\"\"\",\n",
    "        })\n",
    "    return preprocessed_texts\n",
    "\n",
    "def separate_body_label(processed_data):\n",
    "\n",
    "    body_list = list()\n",
    "    label_list = list()\n",
    "\n",
    "    for article in processed_data:\n",
    "        body_list.append(article[\"complete_text\"])\n",
    "        label_list.append(article[\"summary\"])\n",
    "\n",
    "    return body_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b69b6-d9aa-455d-a9d6-874f156d524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training, validation, and test datasets\n",
    "preprocessed_train = process_data(data_train)\n",
    "preprocessed_val = process_data(data_val)\n",
    "preprocessed_test = process_data(data_test)\n",
    "\n",
    "# Create a new list by concatenating preprocessed_train and preprocessed_val\n",
    "merged_list = preprocessed_train + preprocessed_val\n",
    "print(f\"Length of Training: {len(preprocessed_train)}\")\n",
    "print(f\"Length of Validation: {len(preprocessed_val)}\")\n",
    "print(f\"Length of Merged: {len(merged_list)}\")\n",
    "\n",
    "# Seperate the main body and summary of the articles in each dataset\n",
    "training_body, training_label = separate_body_label(preprocessed_train)\n",
    "val_body, val_label = separate_body_label(preprocessed_val)\n",
    "test_body, test_label = separate_body_label(preprocessed_test)\n",
    "merged_body, merged_label = separate_body_label(merged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e124f-490c-4b28-bbfd-fa66ca634063",
   "metadata": {},
   "source": [
    "The merged_list will be used in the final fine-tuning of the model after determining the optimal learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec8a7f-7c8a-42b3-afcd-2ddcb99eaa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ce8a-032c-4121-8a9e-49cbff62c6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730fafab",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pretrained BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define a custom dataset class for your data\n",
    "class BiomedicalDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.data[idx]\n",
    "        target_text = self.labels[idx]\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = input_encoding[\"input_ids\"].squeeze()\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Define your datasets and dataloaders\n",
    "max_len = 800\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = BiomedicalDataset(training_body, training_label, tokenizer, max_length=max_len)\n",
    "val_dataset = BiomedicalDataset(val_body, val_label, tokenizer, max_length=max_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Observe the learning rate and number of epochs\n",
    "learning_rates = [1e-6, 5e-6, 1e-5, 5e-5]\n",
    "num_epochs = 20\n",
    "loss_results = {lr: {\"train_loss\": list(), \"val_loss\": list()} for lr in learning_rates}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_results[lr][\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(model.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "                labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_total_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "        loss_results[lr][\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss}, Avg Val Loss: {avg_val_loss}\")\n",
    "\n",
    "# Plot the validation curves for all learning rates in one plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Define base colors for different learning rates\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.plot(loss_results[lr][\"train_loss\"], label=f'Train Loss (LR={lr})', color=colors[i], linestyle='-', alpha=0.7)\n",
    "    plt.plot(loss_results[lr][\"val_loss\"], label=f'Val Loss (LR={lr})', color=colors[i], linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('Training and Validation Loss by Learning Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ec055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed52437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a288aa32-1efd-4288-8686-c38081c1c3ba",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce5ed7-3e8d-47cf-b803-289f21f504a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for your data\n",
    "class BiomedicalDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.data[idx]\n",
    "        target_text = self.labels[idx]\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = input_encoding[\"input_ids\"].squeeze()\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0798f8-4519-4032-86d3-84dc679058de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your datasets and dataloaders\n",
    "max_len = 800\n",
    "batch_size = 16\n",
    "\n",
    "# Define the optimal hyperparameters\n",
    "learning_rate = 5e-6\n",
    "num_epochs = 6\n",
    "\n",
    "# Load the pretrained BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Use the merged training and validation sets to train the model\n",
    "train_dataset = BiomedicalDataset(merged_body, merged_label, tokenizer, max_length=max_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the test dataset and dataloader\n",
    "test_dataset = BiomedicalDataset(test_body, test_label, tokenizer, max_length=max_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training and test errors per epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    test_loss = evaluate(model, test_dataloader)\n",
    "    print(f\"Epoch-{epoch+1} >>> Training Loss : {avg_train_loss:.3f} & Test Loss : {test_loss:.3f}\")\n",
    "    # Calculate the test error\n",
    "    \n",
    "    print(f\"Test Loss for Epoch-{epoch+1}: {test_loss:.3f}\")\n",
    "\n",
    "# End timing the training process\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"fine_tuned_bart_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_bart_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d3ccf-8eec-4ccc-aca1-577040c26f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e3c20-dcfd-492f-9593-cb2676e67112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400c7b2-1561-4189-8a33-e6a3ca899da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c36dea-0413-458e-bec9-998e1cc2a024",
   "metadata": {},
   "source": [
    "## Check the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9713fc9-9664-4aef-913e-751825103791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current Working Directory: {current_dir}\")\n",
    "\n",
    "# List the contents of the current directory\n",
    "contents = os.listdir(current_dir)\n",
    "print(f\"Contents of {current_dir}: {contents}\")\n",
    "\n",
    "# Define the expected directory name for the fine-tuned model\n",
    "model_dir = \"fine_tuned_bart_model\"\n",
    "\n",
    "# Check if the model directory exists in the current directory\n",
    "if model_dir in contents:\n",
    "    print(f\"The model directory '{model_dir}' is found in the current directory.\")\n",
    "    # List the contents of the model directory\n",
    "    model_contents = os.listdir(os.path.join(current_dir, model_dir))\n",
    "    print(f\"Contents of {model_dir}: {model_contents}\")\n",
    "else:\n",
    "    print(f\"The model directory '{model_dir}' is not found in the current directory. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d457b-99c3-44e0-8082-c7c925a14ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a51180-40bf-4a48-9100-424d01c6b296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cefa0a82-a0d9-4fac-a99c-9bdce11cecd6",
   "metadata": {},
   "source": [
    "# Generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629d030-ec3c-4199-97c2-46513b548dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete summarization\n",
    "def generate_complete(text_list, model, tokenizer, n_beams, min_len, max_len):\n",
    "\n",
    "    summary_list = list()\n",
    "\n",
    "    for article in text_list:\n",
    "\n",
    "        # New article to summarize\n",
    "        new_article = article\n",
    "        \n",
    "        # Tokenize the new article and move the input tensors to the device\n",
    "        inputs = tokenizer(new_article, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Generate Summary\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=n_beams, min_length=min_len, max_length=max_len)\n",
    "        \n",
    "        # Decode Summary\n",
    "        summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "        summary_list.append(summary)\n",
    "\n",
    "    return summary_list\n",
    "\n",
    "# Step-by-step summarization\n",
    "def generate_sbs(text_list, model, n_beams, min_len, max_len):\n",
    "\n",
    "    summary_list = list()\n",
    "    \n",
    "    # Iterate through the preprocessed_test list\n",
    "    for i, text_dict in enumerate(text_list):\n",
    "    \n",
    "        summarized_body = \"\"\n",
    "    \n",
    "        # Add the abstract to summarized_body if it exists\n",
    "        if 'abstract' in text_dict:\n",
    "            summarized_body += text_dict['abstract']\n",
    "        \n",
    "        # Summarize each section except for the excluded keys\n",
    "        for section_key, section_value in text_dict.items():\n",
    "            if section_key not in ['abstract', 'main_body', 'complete_text', 'summary']:\n",
    "                inputs = tokenizer(section_value, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "                inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "                summary_ids = model.generate(inputs[\"input_ids\"], num_beams=n_beams, min_length=min_len, max_length=max_len)\n",
    "                section_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "                # Append the section summary to summarized_body\n",
    "                summarized_body += section_summary\n",
    "    \n",
    "        # Generate a final summary for summarized_body\n",
    "        final_inputs = tokenizer(summarized_body, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "        final_inputs = {key: value.to(device) for key, value in final_inputs.items()}\n",
    "        final_summary_ids = model.generate(final_inputs[\"input_ids\"], num_beams=n_beams, min_length=min_len, max_length=max_len)\n",
    "        final_summary = tokenizer.batch_decode(final_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "        summary_list.append(final_summary)\n",
    "\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d431f61-1f6c-48c7-82f9-7e87cd784897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644914d5-a39e-4a62-b693-1a9e9b2a510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4227dce3-1540-4f24-833d-53a33f1bafc6",
   "metadata": {},
   "source": [
    "## 1) Generate Summaries Using The Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0946762-344c-4b71-b2b3-e8278b6e94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load the fine-tuned BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define model parameters\n",
    "n_beams = 5\n",
    "min_len = 600\n",
    "max_len = 800\n",
    "\n",
    "pretrained_complete = generate_complete(preprocessed_test, model, tokenizer, n_beams, min_len, max_len)\n",
    "pretrained_sbs = generate_sbs(preprocessed_test, model, tokenizer, n_beams, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c9717-186c-4f08-aa05-1d154d57fa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce198370-608e-4cb4-b416-7fcb809a09d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b364a25-6732-4ae7-b876-088b0522c34a",
   "metadata": {},
   "source": [
    "## 2) Generate Summaries Using The Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c4be-4e41-4edc-8811-9dff469c5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load the fine-tuned BART model and tokenizer\n",
    "model_name = \"fine_tuned_bart_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define model parameters\n",
    "n_beams = 5\n",
    "min_len = 600\n",
    "max_len = 800\n",
    "\n",
    "finetuned_complete = generate_complete(preprocessed_test, model, tokenizer, n_beams, min_len, max_len)\n",
    "finetuned_sbs = generate_sbs(preprocessed_test, model, tokenizer, n_beams, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401e589-2fab-4cc9-a08a-885893209ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816576d-d7f2-43b9-b950-a00dcc70f9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "592bde6e-b9bb-4f15-85e2-d1140797d461",
   "metadata": {
    "id": "31dc8280"
   },
   "source": [
    "# Summary Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4badb-d689-4841-bdc8-8dbd9fb630fa",
   "metadata": {},
   "source": [
    "## 1) Factuality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa56b-e542-4fb4-8f88-74a9bec7534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BARTScorer:\n",
    "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):\n",
    "        # Set up model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Set up loss\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        \"\"\" Load model from paraphrase finetuning \"\"\"\n",
    "        if path is None:\n",
    "            path = 'models/bart.pth'\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def score(self, srcs, tgts, batch_size=4):\n",
    "        \"\"\" Score a batch of examples \"\"\"\n",
    "        score_list = []\n",
    "        for i in range(0, len(srcs), batch_size):\n",
    "            src_list = srcs[i: i + batch_size]\n",
    "            tgt_list = tgts[i: i + batch_size]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    encoded_src = self.tokenizer(\n",
    "                        src_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    encoded_tgt = self.tokenizer(\n",
    "                        tgt_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
    "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
    "\n",
    "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
    "                    tgt_mask = encoded_tgt['attention_mask']\n",
    "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
    "\n",
    "                    output = self.model(\n",
    "                        input_ids=src_tokens,\n",
    "                        attention_mask=src_mask,\n",
    "                        labels=tgt_tokens\n",
    "                    )\n",
    "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
    "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "                    loss = loss.sum(dim=1) / tgt_len\n",
    "                    curr_score_list = [-x.item() for x in loss]\n",
    "                    score_list += curr_score_list\n",
    "\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print(f'source: {src_list}')\n",
    "                print(f'target: {tgt_list}')\n",
    "                exit(0)\n",
    "        return score_list\n",
    "\n",
    "    def multi_ref_score(self, srcs, tgts: List[List[str]], agg=\"mean\", batch_size=4):\n",
    "        # Assert we have the same number of references\n",
    "        ref_nums = [len(x) for x in tgts]\n",
    "        if len(set(ref_nums)) > 1:\n",
    "            raise Exception(\"You have different number of references per test sample.\")\n",
    "\n",
    "        ref_num = len(tgts[0])\n",
    "        score_matrix = []\n",
    "        for i in range(ref_num):\n",
    "            curr_tgts = [x[i] for x in tgts]\n",
    "            scores = self.score(srcs, curr_tgts, batch_size)\n",
    "            score_matrix.append(scores)\n",
    "        if agg == \"mean\":\n",
    "            score_list = np.mean(score_matrix, axis=0)\n",
    "        elif agg == \"max\":\n",
    "            score_list = np.max(score_matrix, axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return list(score_list)\n",
    "\n",
    "    def test(self, batch_size=3):\n",
    "        \"\"\" Test \"\"\"\n",
    "        src_list = [\n",
    "            'This is a very good idea. Although simple, but very insightful.',\n",
    "            'Can I take a look?',\n",
    "            'Do not trust him, he is a liar.'\n",
    "        ]\n",
    "\n",
    "        tgt_list = [\n",
    "            \"That's stupid.\",\n",
    "            \"What's the problem?\",\n",
    "            'He is trustworthy.'\n",
    "        ]\n",
    "\n",
    "        print(self.score(src_list, tgt_list, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2915090-6d92-4f2e-a07a-1a92d1e9ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 800\n",
    "\n",
    "# 1) Pre-trained summaries\n",
    "pre_model = 'facebook/bart-large-cnn'\n",
    "# Create an instance of BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda:0', max_length=max_len, checkpoint=pre_model)\n",
    "\n",
    "# Score the generated summaries against the target summaries\n",
    "pretrained_complete_scores = bart_scorer.score(pretrained_complete, list(test_label))\n",
    "pretrained_sbs_scores = bart_scorer.score(pretrained_sbs, list(test_label))\n",
    "print(\"Pre-trained Factuality Scores:\")\n",
    "print(f\"Complete Summaries: {pretrained_complete_scores:.3f}\")\n",
    "print(f\"Step-by-step Summaries: {pretrained_sbs_scores:.3f}\")\n",
    "\n",
    "# 2) Fine-tuned summaries\n",
    "tuned_model = 'fine_tuned_bart_model'\n",
    "# Create an instance of BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda:0', max_length=max_len, checkpoint=tuned_model)\n",
    "\n",
    "# Score the generated summaries against the target summaries\n",
    "finetuned_complete_scores = bart_scorer.score(finetuned_complete, list(test_label))\n",
    "finetuned_sbs_scores = bart_scorer.score(finetuned_sbs, list(test_label))\n",
    "print(\"Pre-trained Factuality Scores:\")\n",
    "print(f\"Complete Summaries: {finetuned_complete_scores:.3f}\")\n",
    "print(f\"Step-by-step Summaries: {finetuned_sbs_scores:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebc81e-c7de-4dbd-9a01-1a3dc9a3cf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7c922-897d-4359-bff8-c79c392bef62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15d2b26-bc46-4ba5-b6b7-bff62d2231a4",
   "metadata": {},
   "source": [
    "## 2) Relevance and Readibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from readability import Readability\n",
    "\n",
    "def relevance_readibility(generated_list, target_list):\n",
    "\n",
    "    text_results = list()\n",
    "\n",
    "    for i in range(len(generated_list)):\n",
    "\n",
    "        generated_sum = generated_list[i]\n",
    "        target_sum = test_label[i]\n",
    "\n",
    "        # 1) Relevance\n",
    "        rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = rouge.score(generated_sum, target_sum)\n",
    "\n",
    "        # Extract F-measures for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "        rouge1_fmeasure = rouge_scores['rouge1'].fmeasure\n",
    "        rouge2_fmeasure = rouge_scores['rouge2'].fmeasure\n",
    "        rougel_fmeasure = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        # Calculate BERT score\n",
    "        P, R, F1 = bert_score([generated_sum], [target_sum], lang='en', model_type='roberta-large')\n",
    "        # F-measure is at index 2\n",
    "        bert_fmeasure = F1[0].item()\n",
    "\n",
    "        # 2) Readability\n",
    "        readability_target = Readability(target_sum)\n",
    "        fkgl_target = readability_target.flesch_kincaid().score\n",
    "\n",
    "        # Calculate FKGL for generated summaries\n",
    "        readability_generated = Readability(generated_sum)\n",
    "        fkgl_generated = readability_generated.flesch_kincaid().score\n",
    "\n",
    "        # Compare readability of generated summaries with target summary\n",
    "        # Calculate absolute difference\n",
    "        diff_A = abs(fkgl_generated - fkgl_target)\n",
    "\n",
    "        result_dict = {\n",
    "            \"Rouge-1 F-measure\": rouge1_fmeasure,\n",
    "            \"Rouge-2 F-measure\": rouge2_fmeasure,\n",
    "            \"Rouge-L F-measure\": rougel_fmeasure,\n",
    "            \"Bert Score F-measure\": bert_fmeasure,\n",
    "            \"Readibility Difference\": diff_A,\n",
    "        }\n",
    "\n",
    "        # Append result dictionary to complete_text_results list\n",
    "        text_results.append(result_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    evaluation_df = pd.DataFrame(text_results)\n",
    "\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee928ff-7c51-419d-b2e2-f5c1f3ffc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_complete_df = relevance_readibility(pretrained_complete, list(test_label))\n",
    "pretrained_sbs_df = relevance_readibility(pretrained_sbs, list(test_label))\n",
    "finetuned_complete_df = relevance_readibility(finetuned_complete, list(test_label))\n",
    "finetuned_sbs_df = relevance_readibility(finetuned_sbs, list(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7de187",
   "metadata": {
    "id": "ec7de187"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd4c5ed",
   "metadata": {
    "id": "31f05ce5"
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
